{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IslamiTP/Intro-to-LLM-Homework-2/blob/main/run_ollama_in_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0MJe6AusVtI"
      },
      "source": [
        "## Run Ollama in Colab\n",
        "\n",
        "### Code to initialize llama using Unsloth was retrieved from\n",
        "\n",
        "https://colab.research.google.com/drive/1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9?usp=sharing#scrollTo=2eSvM9zX_2d3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git@nightly git+https://github.com/unslothai/unsloth-zoo.git"
      ],
      "metadata": {
        "id": "3KJQQP3ZvVI5"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
        "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "\n",
        "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
        "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-kJWXK4vYy8",
        "outputId": "f033ab89-88e9-4c1b-f65d-cdfe8b76b0a3"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.3.11: Fast Llama patching. Transformers: 4.48.3.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 1, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "gzxZE8YozcGW"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.2\",\n",
        ")\n",
        "\n",
        "def formatting_train_prompts(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],  # Use only text\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=2048,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the full dataset\n",
        "full_dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
        "\n",
        "# Split the dataset into training and testing sets (e.g., 80% train, 20% test)\n",
        "split_datasets = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "# Access the train and test splits\n",
        "train_dataset = split_datasets['train']\n",
        "test_dataset = split_datasets['test']\n",
        "\n",
        "print(f\"Train dataset: {train_dataset}\")\n",
        "print(f\"Test dataset: {test_dataset}\")"
      ],
      "metadata": {
        "id": "WHB254cnJB2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eaeab6d-cc10-4f0b-c47c-41ef9431485d"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset: Dataset({\n",
            "    features: ['conversations', 'source', 'score'],\n",
            "    num_rows: 80000\n",
            "})\n",
            "Test dataset: Dataset({\n",
            "    features: ['conversations', 'source', 'score'],\n",
            "    num_rows: 20000\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "train_dataset = standardize_sharegpt(train_dataset)\n",
        "train_dataset = train_dataset.map(formatting_train_prompts, batched = True,)\n",
        "test_dataset = standardize_sharegpt(test_dataset)\n",
        "test_dataset = test_dataset.map(formatting_train_prompts, batched=True)\n",
        "tokenized_datasets = test_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "L5o4EfODWYmy"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before training inference outcome from continue fibonacci sequence question.\n",
        "\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "import torch\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3.2\",\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\\n\\n\"},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,  # Must add for generation\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "# Use torch.no_grad() for inference without locking the model\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs,\n",
        "        max_new_tokens=2048,\n",
        "        use_cache=True,\n",
        "        temperature=1.5,\n",
        "        min_p=0.1\n",
        "    )\n",
        "\n",
        "output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "print(output[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y990-s8batG",
        "outputId": "440f93bc-a418-4d2e-cb00-9425fa563015"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 July 2024\n",
            "\n",
            "user\n",
            "\n",
            "Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\n",
            "\n",
            "assistant\n",
            "\n",
            "The next numbers in the Fibonacci sequence are:\n",
            "\n",
            "13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 121393, 196418, 317811, 514229, 832040, 1346269, 2178309, 3524578, 5702887, 9227465, 14930352, 24157817, 39088169, 63245986, 102334155, 165580141, 267914296, 433494437, 701408733, 1134903170...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question #1\n",
        "#### Grabbing questions from different datasets for text summarization, question answering, text classification, role playing, and reasoning"
      ],
      "metadata": {
        "id": "Ju4PUW3pEja6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching multiple datasets and loading them for inference\n",
        "\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Define a function to format datasets\n",
        "def format_dataset(dataset_name, dataset):\n",
        "    formatted_data = []\n",
        "\n",
        "    if dataset_name == \"cnn_dailymail\" or dataset_name == \"xsum\":\n",
        "        # For summarization datasets, use the content as the article and the role as the summary\n",
        "        for example in dataset[\"train\"]:\n",
        "            formatted_data.append({\n",
        "                \"content\": example[\"article\"],\n",
        "                \"role\": example[\"highlights\"] if dataset_name == \"cnn_dailymail\" else example[\"summary\"]\n",
        "            })\n",
        "\n",
        "    elif dataset_name == \"squad\":\n",
        "        print(dataset.column_names)\n",
        "        # For SQuAD, we need to pair the context (content) with the question (role) and the answer\n",
        "        for example in dataset[\"train\"]:\n",
        "            for qa in example[\"qas\"]:\n",
        "                formatted_data.append({\n",
        "                    \"content\": example[\"context\"],\n",
        "                    \"role\": qa[\"question\"]\n",
        "                })\n",
        "\n",
        "    elif dataset_name == \"natural_questions\":\n",
        "        # For Natural Questions, use the question as role and the context as content\n",
        "        for example in dataset[\"train\"]:\n",
        "            formatted_data.append({\n",
        "                \"content\": example[\"document_text\"],\n",
        "                \"role\": example[\"question_text\"]\n",
        "            })\n",
        "\n",
        "    elif dataset_name == \"ag_news\":\n",
        "        # For text classification datasets, the content is the news article and the role is the label\n",
        "        for example in dataset[\"train\"]:\n",
        "            formatted_data.append({\n",
        "                \"content\": example[\"text\"],\n",
        "                \"role\": example[\"label\"]\n",
        "            })\n",
        "\n",
        "    elif dataset_name == \"imdb\":\n",
        "        # For sentiment analysis, the content is the review and the role is the sentiment (positive/negative)\n",
        "        for example in dataset[\"train\"]:\n",
        "            formatted_data.append({\n",
        "                \"content\": example[\"text\"],\n",
        "                \"role\": example[\"label\"]\n",
        "            })\n",
        "\n",
        "    elif dataset_name == \"daily_dialog\":\n",
        "        # For dialog datasets, we need to pair the dialogue (content) with the speaker role\n",
        "        for example in dataset[\"train\"]:\n",
        "            for i, utt in enumerate(example[\"dialog\"]):\n",
        "                role = \"user\" if i % 2 == 0 else \"agent\"\n",
        "                formatted_data.append({\n",
        "                    \"content\": utt,\n",
        "                    \"role\": role\n",
        "                })\n",
        "\n",
        "    elif dataset_name == \"persona_chat\":\n",
        "        # For persona chat, content is the dialogue and role is the persona-based speaker\n",
        "        for example in dataset[\"train\"]:\n",
        "            for i, utt in enumerate(example[\"dialog\"]):\n",
        "                role = \"user\" if i % 2 == 0 else \"persona\"\n",
        "                formatted_data.append({\n",
        "                    \"content\": utt,\n",
        "                    \"role\": role\n",
        "                })\n",
        "\n",
        "    elif dataset_name == \"piqa\":\n",
        "        # For PIQA, the content is the question and the role is the answer\n",
        "        for example in dataset[\"train\"]:\n",
        "            formatted_data.append({\n",
        "                \"content\": example[\"question\"],\n",
        "                \"role\": example[\"answerA\"]  # You can choose answerA or answerB depending on your use case\n",
        "            })\n",
        "\n",
        "    elif dataset_name == \"commonsense_qa\":\n",
        "        # For commonsense QA, the content is the question and the role is the answer\n",
        "        for example in dataset[\"train\"]:\n",
        "            formatted_data.append({\n",
        "                \"content\": example[\"question\"],\n",
        "                \"role\": example[\"answer\"]\n",
        "            })\n",
        "\n",
        "    return formatted_data\n",
        "\n",
        "# List of datasets to load\n",
        "datasets = [\"cnn_dailymail\", \"squad\", \"natural_questions\", \"ag_news\",\n",
        "            \"imdb\", \"daily_dialog\", \"persona_chat\", \"piqa\", \"commonsense_qa\"]\n",
        "\n",
        "# Load and format all datasets\n",
        "formatted_datasets = {}\n",
        "for dataset_name in datasets:\n",
        "    print(f\"Loading and formatting {dataset_name}...\")\n",
        "    # Load the dataset\n",
        "    dataset = load_dataset(dataset_name, '3.0.0') if dataset_name == \"cnn_dailymail\" else load_dataset(dataset_name)\n",
        "\n",
        "    # Format the dataset\n",
        "    formatted_data = format_dataset(dataset_name, dataset)\n",
        "\n",
        "    # Store the formatted data in a dictionary\n",
        "    formatted_datasets[dataset_name] = formatted_data\n",
        "\n",
        "# Now, `formatted_datasets` contains all datasets in the desired format: {\"content\": \"\", \"role\": \"\"}\n",
        "# Example usage:\n",
        "# Print the first entry from the IMDB dataset\n",
        "print(formatted_datasets[\"imdb\"][0])\n",
        "\n",
        "# You can also convert them into DataFrames for easier handling\n",
        "for dataset_name, data in formatted_datasets.items():\n",
        "    df = pd.DataFrame(data)\n",
        "    print(f\"First few entries of {dataset_name}:\")\n",
        "    print(df.head())"
      ],
      "metadata": {
        "id": "8X_KG1YzEg1y",
        "outputId": "08e3caaa-8ed6-4c62-82b5-c84ce049506f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and formatting cnn_dailymail...\n",
            "Loading and formatting squad...\n",
            "{'train': ['id', 'title', 'context', 'question', 'answers'], 'validation': ['id', 'title', 'context', 'question', 'answers']}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'qas'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-125-1e042c49a1c0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;31m# Format the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mformatted_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# Store the formatted data in a dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-125-1e042c49a1c0>\u001b[0m in \u001b[0;36mformat_dataset\u001b[0;34m(dataset_name, dataset)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# For SQuAD, we need to pair the context (content) with the question (role) and the answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mqa\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"qas\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 formatted_data.append({\n\u001b[1;32m     24\u001b[0m                     \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'qas'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question #2\n",
        "#### Fine-tuning using LORA and SFTTrainer which is a special trainer designed to fine-tune models."
      ],
      "metadata": {
        "id": "ZYF6A_tTFwrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None  # Auto detection, change if needed\n",
        "load_in_4bit = False  # Set to True if using 4bit quantization\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    dataset_text_field = \"text\",  # Keep this as \"text\" since that's what formatting_func returns\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        learning_rate = 9e-4,\n",
        "        max_steps=60,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0OJDFB0eD4e",
        "outputId": "1263e1d0-4545-4a0e-dc8b-5b5d18ad1be5"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: We found double BOS tokens - we shall remove one automatically.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you already have the model, test_dataset, and trainer set up\n",
        "\n",
        "# Calculate the test loss before training\n",
        "trainer.model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "before_training_results = trainer.evaluate(tokenized_datasets.select(range(200)))\n",
        "print(before_training_results)\n",
        "before_training_loss = before_training_results[\"eval_loss\"]\n",
        "\n",
        "print(train_dataset.column_names)\n",
        "# Train the model\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Calculate the test loss after training\n",
        "trainer.model.eval()  # Set the model back to evaluation mode\n",
        "after_training_results = trainer.evaluate(tokenized_datasets.select(range(200)))\n",
        "after_training_loss = after_training_results[\"eval_loss\"]\n",
        "\n",
        "# Prepare the data for the bar plot\n",
        "losses = [before_training_loss, after_training_loss]\n",
        "labels = ['Before Training', 'After Training']\n",
        "\n",
        "# Create the bar plot\n",
        "plt.bar(labels, losses, color=['blue', 'green'])\n",
        "plt.xlabel('Stage')\n",
        "plt.ylabel('Test Loss')\n",
        "plt.title('Test Loss Before and After Training')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dqy06i9_uCjt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a7caf8c2-033c-41fd-8f14-312f96714618"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 02:57]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 1.2571914196014404, 'eval_model_preparation_time': 0.007, 'eval_runtime': 48.7897, 'eval_samples_per_second': 4.099, 'eval_steps_per_second': 0.512}\n",
            "['conversations', 'source', 'score', 'text']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 80,000 | Num Epochs = 1 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 1,519,616/3,214,269,440 (0.05% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 01:18, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.951800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.018900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.985800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.987500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.867400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.879400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.659000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.668700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.725300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.808200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.662000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.590400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.589400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.766400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.615500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.581800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.451200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.525900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.471500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.739500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.844500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.942400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.863700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.075300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.695800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.554100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.696700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.804300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.028300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.665900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.703200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.912800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.894800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.901000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.907000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.687100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.886800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.792100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.725200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.734200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.785400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.221600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.655200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.798100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.926000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.078900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.814600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.894000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.821100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.921800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.720700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.826100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.938700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.981700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.631400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.992600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.671600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.665800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.915300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training inference outcome from continue fibonacci sequence question.\n",
        "\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.2\",\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\\n\\n\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = max_seq_length, use_cache = True,\n",
        "                         temperature = 1.5, min_p = 0.1)\n",
        "\n",
        "output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "print(output[0])"
      ],
      "metadata": {
        "id": "HHK1IvWv_8I1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}