{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IslamiTP/Intro-to-LLM-Homework-2/blob/main/run_ollama_in_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0MJe6AusVtI"
      },
      "source": [
        "## Run Ollama in Colab\n",
        "\n",
        "### Code to initialize llama using Unsloth was retrieved from\n",
        "\n",
        "https://colab.research.google.com/drive/1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9?usp=sharing#scrollTo=2eSvM9zX_2d3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth"
      ],
      "metadata": {
        "id": "3KJQQP3ZvVI5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
        "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "\n",
        "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
        "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\" # NEW! Llama 3.3 70B!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-kJWXK4vYy8",
        "outputId": "aae8e178-508d-4854-be0b-3d04f7e5e375"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.3.14: Fast Llama patching. Transformers: 4.48.3.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 1, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "gzxZE8YozcGW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.2\",\n",
        ")\n",
        "\n",
        "def formatting_train_prompts(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],  # Use only text\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=2048,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the full dataset\n",
        "full_dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
        "\n",
        "# Split the dataset into training and testing sets (e.g., 80% train, 20% test)\n",
        "split_datasets = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "# Access the train and test splits\n",
        "train_dataset = split_datasets['train']\n",
        "test_dataset = split_datasets['test']\n",
        "\n",
        "print(f\"Train dataset: {train_dataset}\")\n",
        "print(f\"Test dataset: {test_dataset}\")"
      ],
      "metadata": {
        "id": "WHB254cnJB2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed1d147-ef38-4c46-db1b-748cea507ad2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset: Dataset({\n",
            "    features: ['conversations', 'source', 'score'],\n",
            "    num_rows: 80000\n",
            "})\n",
            "Test dataset: Dataset({\n",
            "    features: ['conversations', 'source', 'score'],\n",
            "    num_rows: 20000\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "train_dataset = standardize_sharegpt(train_dataset)\n",
        "train_dataset = train_dataset.map(formatting_train_prompts, batched = True,)\n",
        "test_dataset = standardize_sharegpt(test_dataset)\n",
        "test_dataset = test_dataset.map(formatting_train_prompts, batched=True)\n",
        "tokenized_datasets = test_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "L5o4EfODWYmy"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "import torch\n",
        "\n",
        "# Assuming tokenizer and model are already loaded\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3.2\",\n",
        ")\n",
        "\n",
        "# Define a list of messages (each entry follows the chat template format)\n",
        "\n",
        "\n",
        "message_list = [\n",
        "    [{\"role\": \"system\", \"content\": \"You are a helpful AI assistant that specializes in summarizing text.\\n\\n\"}, # Text Summarization\n",
        "     {\"role\": \"user\", \"content\": \"Summarize the following article in 2-3 sentences:\\n\\nIn recent years, the importance of renewable energy has grown significantly due to increasing concerns about climate change. Solar and wind energy are leading the way in the shift towards cleaner energy sources. Governments worldwide are investing heavily in renewable energy technologies, and many countries are setting ambitious goals for reducing carbon emissions. The transition to renewable energy is seen as crucial to mitigating the impacts of climate change and ensuring a sustainable future for generations to come.\\n\\n\"}],\n",
        "\n",
        "    [{\"role\": \"system\", \"content\": \"You are an AI assistant that answers questions based on provided passages.\\n\\n\"}, # Question Answering\n",
        "     {\"role\": \"user\", \"content\": \"Read the following passage and answer the question:\\n\\nThe Eiffel Tower is located in Paris, France. It was completed in 1889 for the World's Fair and was originally intended as a temporary structure. However, it became one of the most iconic landmarks in the world, attracting millions of visitors each year.\\n\\nQuestion: When was the Eiffel Tower completed?\\n\\n\"}],\n",
        "\n",
        "    [{\"role\": \"system\", \"content\": \"You are an AI that classifies text into categories: Technology, Health, Sports, or Politics.\\n\\n\"}, # Text Classification\n",
        "     {\"role\": \"user\", \"content\": \"Classify the following text:\\n\\nThe latest advancements in AI and machine learning are revolutionizing industries. Companies are now using AI to improve efficiency and productivity in various sectors, from healthcare to finance.\\n\\n\"}],\n",
        "\n",
        "    [{\"role\": \"system\", \"content\": \"You are a customer support agent for an online course company. Respond helpfully to customers.\\n\\n\"}, # Role Playing\n",
        "     {\"role\": \"user\", \"content\": \"Hi, I’m having trouble accessing the course I purchased. Can you help me?\\n\\n\"}],\n",
        "\n",
        "    [{\"role\": \"system\", \"content\": \"Answer the following reasoning question:\\n\\n\"}, #Reasoning\n",
        "     {\"role\": \"user\", \"content\": \"If all roses are flowers, and some flowers are red, can we conclude that some roses are red?\\n\\n\"}]\n",
        "]\n",
        "\n",
        "# Store results\n",
        "generated_outputs = []\n",
        "\n",
        "# Iterate through message list and generate responses\n",
        "for messages in message_list:\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,  # Required for generation\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Run inference without gradient tracking\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            max_new_tokens=2048,\n",
        "            use_cache=True,\n",
        "            temperature=1.5,\n",
        "            min_p=0.1\n",
        "        )\n",
        "\n",
        "    # Decode and store the output\n",
        "    output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    generated_outputs.append(output_text)\n",
        "\n",
        "prompt_types = [\"Text Summarization\", \"Question Answering\", \"Text Classification\", \"Role Playing\", \"Reasoning\"]\n",
        "\n",
        "# Print all generated responses\n",
        "for i, (response, messages, prompt_type) in enumerate(zip(generated_outputs, message_list, prompt_types)):\n",
        "    print(f\"Prompt Type: {prompt_type}\\n\\n\")\n",
        "\n",
        "    print(f\"Response: {response}\\n\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y990-s8batG",
        "outputId": "22955197-9e2b-4458-df1d-2fd33437d59b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt Type: Text Summarization\n",
            "\n",
            "\n",
            "Response: system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 July 2024\n",
            "\n",
            "You are a helpful AI assistant that specializes in summarizing text.\n",
            "\n",
            "user\n",
            "\n",
            "Summarize the following article in 2-3 sentences:\n",
            "\n",
            "In recent years, the importance of renewable energy has grown significantly due to increasing concerns about climate change. Solar and wind energy are leading the way in the shift towards cleaner energy sources. Governments worldwide are investing heavily in renewable energy technologies, and many countries are setting ambitious goals for reducing carbon emissions. The transition to renewable energy is seen as crucial to mitigating the impacts of climate change and ensuring a sustainable future for generations to come.\n",
            "\n",
            "assistant\n",
            "\n",
            "Here is a 2-3 sentence summary of the article:\n",
            "\n",
            "The importance of renewable energy has significantly grown due to climate change concerns. Solar and wind energy are at the forefront of the shift towards cleaner energy sources, with governments worldwide investing heavily in this field. The transition to renewable energy is crucial for mitigating climate change and securing a sustainable future.\n",
            "\n",
            "\n",
            "\n",
            "Prompt Type: Question Answering\n",
            "\n",
            "\n",
            "Response: system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 July 2024\n",
            "\n",
            "You are an AI assistant that answers questions based on provided passages.\n",
            "\n",
            "user\n",
            "\n",
            "Read the following passage and answer the question:\n",
            "\n",
            "The Eiffel Tower is located in Paris, France. It was completed in 1889 for the World's Fair and was originally intended as a temporary structure. However, it became one of the most iconic landmarks in the world, attracting millions of visitors each year.\n",
            "\n",
            "Question: When was the Eiffel Tower completed?\n",
            "\n",
            "assistant\n",
            "\n",
            "According to the passage, the Eiffel Tower was completed in 1889.\n",
            "\n",
            "\n",
            "\n",
            "Prompt Type: Text Classification\n",
            "\n",
            "\n",
            "Response: system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 July 2024\n",
            "\n",
            "You are an AI that classifies text into categories: Technology, Health, Sports, or Politics.\n",
            "\n",
            "user\n",
            "\n",
            "Classify the following text:\n",
            "\n",
            "The latest advancements in AI and machine learning are revolutionizing industries. Companies are now using AI to improve efficiency and productivity in various sectors, from healthcare to finance.\n",
            "\n",
            "assistant\n",
            "\n",
            "I would classify the text as: Technology\n",
            "\n",
            "\n",
            "\n",
            "Prompt Type: Role Playing\n",
            "\n",
            "\n",
            "Response: system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 July 2024\n",
            "\n",
            "You are a customer support agent for an online course company. Respond helpfully to customers.\n",
            "\n",
            "user\n",
            "\n",
            "Hi, I’m having trouble accessing the course I purchased. Can you help me?\n",
            "\n",
            "assistant\n",
            "\n",
            "I'd be happy to help you access the course. Can you please provide me with some more details so I can assist you better? \n",
            "\n",
            "Here are a few questions to get started:\n",
            "\n",
            "- What is your course name?\n",
            "- What browser and device are you using to access the course?\n",
            "- Have you tried logging in to the course using the email address and password associated with your purchase?\n",
            "- If you're trying to access the course through a subscription plan, please note that it may take 24 hours for new courses to become available in your library.\n",
            "\n",
            "If you have already tried the above, we can further troubleshoot together to resolve any issues and get you access to the course.\n",
            "\n",
            "\n",
            "\n",
            "Prompt Type: Reasoning\n",
            "\n",
            "\n",
            "Response: system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 July 2024\n",
            "\n",
            "Answer the following reasoning question:\n",
            "\n",
            "user\n",
            "\n",
            "If all roses are flowers, and some flowers are red, can we conclude that some roses are red?\n",
            "\n",
            "assistant\n",
            "\n",
            "No, we cannot conclude that some roses are red. \n",
            "\n",
            "The information given is:\n",
            "1. All roses are flowers. (A)\n",
            "2. Some flowers are red. (B)\n",
            "\n",
            "However, the conclusion we can draw is that all flowers are red that are roses, or rather, the flowers that are roses are the same flowers that are red (B is true for some roses in set A), but it does not mean that some roses are red.\n",
            "\n",
            "For a valid conclusion to be drawn from B, we would need another statement linking flowers and roses that are red directly, not just general information that roses and flowers are both flowers.\n",
            "\n",
            "We could say the set of 'roses' is a subset of the set 'flowers' (A), and since some flowers are red (B), it means that at least some 'roses' are red because roses are a subset of flowers and the properties of roses being part of the flowers subset implies they can be classified under 'flowers' that are red.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question #1\n",
        "#### Grabbing questions from different datasets for text summarization, question answering, text classification, role playing, and reasoning"
      ],
      "metadata": {
        "id": "Ju4PUW3pEja6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question #2\n",
        "#### Fine-tuning using LORA and SFTTrainer which is a special trainer designed to fine-tune models."
      ],
      "metadata": {
        "id": "ZYF6A_tTFwrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None  # Auto detection, change if needed\n",
        "load_in_4bit = False  # Set to True if using 4bit quantization\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    dataset_text_field = \"text\",  # Keep this as \"text\" since that's what formatting_func returns\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        learning_rate = 9e-4,\n",
        "        max_steps=60,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0OJDFB0eD4e",
        "outputId": "2788fc15-c4b2-4a4a-d937-b8ed5b485052"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: We found double BOS tokens - we shall remove one automatically.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you already have the model, test_dataset, and trainer set up\n",
        "\n",
        "# Calculate the test loss before training\n",
        "trainer.model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "before_training_results = trainer.evaluate(tokenized_datasets.select(range(200)))\n",
        "print(before_training_results)\n",
        "before_training_loss = before_training_results[\"eval_loss\"]\n",
        "\n",
        "print(train_dataset.column_names)\n",
        "# Train the model\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Calculate the test loss after training\n",
        "trainer.model.eval()  # Set the model back to evaluation mode\n",
        "after_training_results = trainer.evaluate(tokenized_datasets.select(range(200)))\n",
        "after_training_loss = after_training_results[\"eval_loss\"]\n",
        "\n",
        "# Prepare the data for the bar plot\n",
        "losses = [before_training_loss, after_training_loss]\n",
        "labels = ['Before Training', 'After Training']\n",
        "\n",
        "# Create the bar plot\n",
        "plt.bar(labels, losses, color=['blue', 'green'])\n",
        "plt.xlabel('Stage')\n",
        "plt.ylabel('Test Loss')\n",
        "plt.title('Test Loss Before and After Training')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dqy06i9_uCjt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc715bcd-dcc4-407b-e59b-f900a6f48b9f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 02:57]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 1.4238659143447876, 'eval_model_preparation_time': 0.007, 'eval_runtime': 48.7568, 'eval_samples_per_second': 4.102, 'eval_steps_per_second': 0.513}\n",
            "['conversations', 'source', 'score', 'text']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 80,000 | Num Epochs = 1 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 1,519,616/3,214,269,440 (0.05% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 01:18, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.227900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.273900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.276000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.224700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.085000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.033800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.812000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.841000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.916000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.946500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.837100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.770100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.789200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.887700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.775800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.671000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.585900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.682600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.599000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.784700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.963600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.025900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.937400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.176000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.796600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.629400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.817400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.871900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.148800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.715400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.554600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.772600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.987800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.013200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.988800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.010200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.731000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.937900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.849700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.784600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.778100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.835700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.270000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.679200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.813200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.947400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.082100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.823400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.899900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.825700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.929100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.725800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.837000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.918900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.986200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.622000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.001600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.660100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.653100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.911100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "import torch\n",
        "\n",
        "# Assuming tokenizer and model are already loaded\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3.2\",\n",
        ")\n",
        "\n",
        "# Define a list of messages (each entry follows the chat template format)\n",
        "\n",
        "message_list = [\n",
        "    [{\"role\": \"system\", \"content\": \"You are a helpful AI assistant that specializes in summarizing text.\\n\\n\"}, # Text Summarization\n",
        "     {\"role\": \"user\", \"content\": \"Summarize the following article in 2-3 sentences:\\n\\nIn recent years, the importance of renewable energy has grown significantly due to increasing concerns about climate change. Solar and wind energy are leading the way in the shift towards cleaner energy sources. Governments worldwide are investing heavily in renewable energy technologies, and many countries are setting ambitious goals for reducing carbon emissions. The transition to renewable energy is seen as crucial to mitigating the impacts of climate change and ensuring a sustainable future for generations to come.\\n\\n\"}],\n",
        "\n",
        "    [{\"role\": \"system\", \"content\": \"You are an AI assistant that answers questions based on provided passages.\\n\\n\"}, # Question Answering\n",
        "     {\"role\": \"user\", \"content\": \"Read the following passage and answer the question:\\n\\nThe Eiffel Tower is located in Paris, France. It was completed in 1889 for the World's Fair and was originally intended as a temporary structure. However, it became one of the most iconic landmarks in the world, attracting millions of visitors each year.\\n\\nQuestion: When was the Eiffel Tower completed?\\n\\n\"}],\n",
        "\n",
        "    [{\"role\": \"system\", \"content\": \"You are an AI that classifies text into categories: Technology, Health, Sports, or Politics.\\n\\n\"}, # Text Classification\n",
        "     {\"role\": \"user\", \"content\": \"Classify the following text:\\n\\nThe latest advancements in AI and machine learning are revolutionizing industries. Companies are now using AI to improve efficiency and productivity in various sectors, from healthcare to finance.\\n\\n\"}],\n",
        "\n",
        "    [{\"role\": \"system\", \"content\": \"You are a customer support agent for an online course company. Respond helpfully to customers.\\n\\n\"}, # Role Playing\n",
        "     {\"role\": \"user\", \"content\": \"Hi, I’m having trouble accessing the course I purchased. Can you help me?\\n\\n\"}],\n",
        "\n",
        "    [{\"role\": \"system\", \"content\": \"Answer the following reasoning question:\\n\\n\"}, #Reasoning\n",
        "     {\"role\": \"user\", \"content\": \"If all roses are flowers, and some flowers are red, can we conclude that some roses are red?\\n\\n\"}]\n",
        "]\n",
        "\n",
        "# Store results\n",
        "generated_outputs = []\n",
        "\n",
        "# Iterate through message list and generate responses\n",
        "for messages in message_list:\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,  # Required for generation\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Run inference without gradient tracking\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            max_new_tokens=2048,\n",
        "            use_cache=True,\n",
        "            temperature=1.5,\n",
        "            min_p=0.1\n",
        "        )\n",
        "\n",
        "    # Decode and store the output\n",
        "    output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    generated_outputs.append(output_text)\n",
        "\n",
        "prompt_types = [\"Text Summarization\", \"Question Answering\", \"Text Classification\", \"Role Playing\", \"Reasoning\"]\n",
        "\n",
        "# Print all generated responses\n",
        "for i, (response, messages, prompt_type) in enumerate(zip(generated_outputs, message_list, prompt_types)):\n",
        "    print(f\"Prompt Type: {prompt_type}\\n\\n\")\n",
        "\n",
        "    print(f\"Response: {response}\\n\\n\\n\")"
      ],
      "metadata": {
        "id": "HHK1IvWv_8I1",
        "outputId": "228fb55a-4dd1-491d-d93d-8157860dd1f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt Type: Text Summarization\n",
            "\n",
            "\n",
            "Response: system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 July 2024\n",
            "\n",
            "You are a helpful AI assistant that specializes in summarizing text.\n",
            "\n",
            "user\n",
            "\n",
            "Summarize the following article in 2-3 sentences:\n",
            "\n",
            "In recent years, the importance of renewable energy has grown significantly due to increasing concerns about climate change. Solar and wind energy are leading the way in the shift towards cleaner energy sources. Governments worldwide are investing heavily in renewable energy technologies, and many countries are setting ambitious goals for reducing carbon emissions. The transition to renewable energy is seen as crucial to mitigating the impacts of climate change and ensuring a sustainable future for generations to come.\n",
            "\n",
            "assistant\n",
            "\n",
            "Here is a 2-3 sentence summary of the article:\n",
            "\n",
            "The growing importance of renewable energy is a major response to climate change. Solar and wind energy are at the forefront of the shift towards cleaner energy sources. Many governments are investing heavily in renewable energy technologies, with a goal of reducing carbon emissions.\n",
            "\n",
            "\n",
            "\n",
            "Prompt Type: Question Answering\n",
            "\n",
            "\n",
            "Response: system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 July 2024\n",
            "\n",
            "You are an AI assistant that answers questions based on provided passages.\n",
            "\n",
            "user\n",
            "\n",
            "Read the following passage and answer the question:\n",
            "\n",
            "The Eiffel Tower is located in Paris, France. It was completed in 1889 for the World's Fair and was originally intended as a temporary structure. However, it became one of the most iconic landmarks in the world, attracting millions of visitors each year.\n",
            "\n",
            "Question: When was the Eiffel Tower completed?\n",
            "\n",
            "assistant\n",
            "\n",
            "The Eiffel Tower was completed in 1889.\n",
            "\n",
            "Explanation:\n",
            "The passage states that the Eiffel Tower was completed in 1889, which directly answers the question about the completion year of the Eiffel Tower.\n",
            "\n",
            "This answer shows that the user understands how to identify the completion date mentioned in the passage, and it demonstrates the ability to extract relevant information from the provided text.\n",
            "\n",
            "\n",
            "\n",
            "Prompt Type: Text Classification\n",
            "\n",
            "\n",
            "Response: system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 July 2024\n",
            "\n",
            "You are an AI that classifies text into categories: Technology, Health, Sports, or Politics.\n",
            "\n",
            "user\n",
            "\n",
            "Classify the following text:\n",
            "\n",
            "The latest advancements in AI and machine learning are revolutionizing industries. Companies are now using AI to improve efficiency and productivity in various sectors, from healthcare to finance.\n",
            "\n",
            "assistant\n",
            "\n",
            "The text can be classified into the category of Technology.\n",
            "\n",
            "\n",
            "\n",
            "Prompt Type: Role Playing\n",
            "\n",
            "\n",
            "Response: system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 July 2024\n",
            "\n",
            "You are a customer support agent for an online course company. Respond helpfully to customers.\n",
            "\n",
            "user\n",
            "\n",
            "Hi, I’m having trouble accessing the course I purchased. Can you help me?\n",
            "\n",
            "assistant\n",
            "\n",
            "Sorry to hear that you are having trouble with accessing the course. Can you please tell me the name of the course and the date you purchased it? Also, do you receive any error messages when you try to access the course? Additionally, what type of device are you using to access the course, such as a smartphone, tablet, or computer?\n",
            "\n",
            "This information will help me diagnose the issue and assist you in getting back to studying.\n",
            "\n",
            "\n",
            "\n",
            "Prompt Type: Reasoning\n",
            "\n",
            "\n",
            "Response: system\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 July 2024\n",
            "\n",
            "Answer the following reasoning question:\n",
            "\n",
            "user\n",
            "\n",
            "If all roses are flowers, and some flowers are red, can we conclude that some roses are red?\n",
            "\n",
            "assistant\n",
            "\n",
            "Yes, we can.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}